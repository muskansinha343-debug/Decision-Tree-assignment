{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Assignment"
      ],
      "metadata": {
        "id": "kHKTiE0k970g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "Answer:\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression, but it is most commonly used for classification tasks.\n",
        "It works by splitting the dataset into smaller subsets based on the most significant features, forming a tree-like structure of decision rules.\n",
        "\n",
        "How it works (for classification):\n",
        "\n",
        "1. **Start at the root node**:\n",
        "The algorithm looks at all features and selects the best feature to split the data.\n",
        "\n",
        "* ‚ÄúBest‚Äù means the feature that gives the purest split, measured using impurity metrics like\n",
        "\n",
        "* Gini Index\n",
        "\n",
        "* Entropy (Information Gain)\n",
        "\n",
        "2. **Create branches based on the feature split**:\n",
        "The data is divided into subsets depending on the feature values.\n",
        "\n",
        "3. **Repeat splitting**:\n",
        "Each subset again splits on the best feature remaining.\n",
        "This continues until any stopping condition is reached (e.g., no feature left, nodes become pure).\n",
        "\n",
        "4. **Leaf Node (Final Decision)**:\n",
        "Once the tree cannot be split further, the node becomes a leaf, which represents a class label.\n",
        "\n",
        "**Example (simple**):\n",
        "\n",
        "If the feature is \"Age\" and the rule is Age < 18, then the branches will classify individuals into categories (e.g., ‚ÄúMinor‚Äù vs ‚ÄúAdult‚Äù)."
      ],
      "metadata": {
        "id": "BdSqv_sd-IHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "Answer:\n",
        "\n",
        "In Decision Trees, impurity measures are used to decide which feature gives the best split. Two common impurity measures are Gini Impurity and Entropy.\n",
        "\n",
        "1. Gini Impurity\n",
        "Definition:\n",
        "\n",
        "Gini Impurity measures how often a randomly chosen sample would be incorrectly classified if it were labeled randomly according to the class distribution in the node.\n",
        "\n",
        "Formula:\n",
        "Gini\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "Gini=1‚àí\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "2\n",
        "\n",
        "\t‚Äã\n",
        "Where:\n",
        "\n",
        "* ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " = proportion of class i\n",
        "\n",
        "* k = number of classes\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* Gini = 0 ‚Üí perfectly pure node (only one class).\n",
        "\n",
        "* Higher values ‚Üí higher impurity.\n",
        "\n",
        "2. Entropy (Information Gain)\n",
        "Definition:\n",
        "\n",
        "Entropy measures the uncertainty or disorder in a dataset.\n",
        "\n",
        "Formula:\n",
        "Entropy\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "(\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "Entropy=‚àí\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "(p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "Interpretation:\n",
        "\n",
        "Entropy = 0 ‚Üí pure node\n",
        "\n",
        "Maximum when classes are evenly mixed (high disorder)\n",
        "\n",
        "Information Gain is:\n",
        "\n",
        "ùêº\n",
        "ùê∫\n",
        "=\n",
        "Entropy(parent)\n",
        "‚àí\n",
        "Weighted Entropy(children)\n",
        "IG=Entropy(parent)‚àíWeighted Entropy(children)\n",
        "\n",
        "How they impact Decision Tree splits:\n",
        "\n",
        "The algorithm evaluates each feature and calculates impurity before and after the split.\n",
        "\n",
        "It selects the feature that reduces impurity the most:\n",
        "\n",
        "CART algorithm ‚Üí uses Gini Impurity\n",
        "\n",
        "ID3/C4.5 ‚Üí use Entropy (Information Gain)\n",
        "\n",
        "Impact:\n",
        "\n",
        "A split is better if it results in:\n",
        "\n",
        "lower Gini\n",
        "\n",
        "or higher Information Gain\n",
        "\n",
        "Both measures aim to create pure child nodes, meaning the split groups similar classes together."
      ],
      "metadata": {
        "id": "qCPaFGav-Cab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of each.\n",
        "Answer:\n",
        "\n",
        "Decision Trees can easily become overfitted, so pruning techniques are used to control tree growth.\n",
        "There are two types of pruning: Pre-Pruning and Post-Pruning.\n",
        "\n",
        "1. **Pre-Pruning (Early Stopping)\n",
        "Meaning**:\n",
        "\n",
        "Pre-pruning stops the tree from growing too deep by applying constraints during the tree-building process.\n",
        "\n",
        "Examples of pre-pruning rules:\n",
        "\n",
        "Maximum depth (max_depth)\n",
        "\n",
        "Minimum samples to split (min_samples_split)\n",
        "\n",
        "Minimum samples per leaf (min_samples_leaf)\n",
        "\n",
        "Minimum impurity decrease\n",
        "\n",
        "**Advantage**:\n",
        "\n",
        "‚úîÔ∏è Reduces training time because the tree does not grow fully.\n",
        "(Useful when dataset is large.)\n",
        "\n",
        "2. **Post-Pruning (Prune After Full Growth)\n",
        "Meaning**:\n",
        "\n",
        "Post-pruning allows the tree to grow completely, and then removes unnecessary or weak branches afterward.\n",
        "\n",
        "**Common methods**:\n",
        "\n",
        "Cost Complexity Pruning (CCP)\n",
        "\n",
        "Reduced Error Pruning\n",
        "\n",
        "**Advantage**:\n",
        "\n",
        "‚úîÔ∏è Improves accuracy and generalization because it removes overfitted branches while keeping useful splits.\n",
        "(Usually results in a more reliable model.)"
      ],
      "metadata": {
        "id": "tNwRKJXCAYuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "Answer:\n",
        "\n",
        "Information Gain (IG) is a metric used in Decision Trees to measure how much uncertainty (entropy) is reduced after splitting the dataset based on a feature.\n",
        "\n",
        "It helps determine which feature is the best for splitting at each node.\n",
        "\n",
        "**Definition**:\n",
        "\n",
        "Information Gain is defined as the difference between the entropy of the parent node and the weighted entropy of the child nodes after the split.\n",
        "\n",
        "Formula:\n",
        "Information Gain\n",
        "=\n",
        "Entropy(parent)\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "ùëõ\n",
        "ùëñ\n",
        "ùëõ\n",
        "√ó\n",
        "Entropy(child\n",
        "ùëñ\n",
        ")\n",
        "Information Gain=Entropy(parent)‚àí\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        "n\n",
        "n\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "√óEntropy(child\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "\n",
        "* n\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " = number of samples in child node i\n",
        "\n",
        "* ùëõ\n",
        "n = total samples in parent node\n",
        "\n",
        "Entropy = measure of impurity/disorder\n",
        "\n",
        "**Why is Information Gain important?**\n",
        "\n",
        "1. **Helps choose the best feature for splitting**:\n",
        "The feature with the highest Information Gain creates the purest child nodes.\n",
        "\n",
        "2. **Reduces uncertainty**:\n",
        "Higher IG means the split reduces more randomness in class labels.\n",
        "\n",
        "3. **Improves accuracy**:\n",
        "Better splits lead to more accurate and generalized decision trees.\n",
        "\n",
        "4. **Guides tree growth**:\n",
        "IG ensures the tree grows in a meaningful direction by selecting informative features."
      ],
      "metadata": {
        "id": "pQlW1DSJA68W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Decision Trees are widely used in various industries because they are simple, interpretable, and effective for both classification and regression tasks.\n",
        "\n",
        "**Real-world applications of Decision Trees:**\n",
        "\n",
        "1. **Medical Diagnosis:**\n",
        "Classifying diseases based on symptoms, test reports, and patient history.\n",
        "\n",
        "2. **Banking & Finance:**\n",
        "\n",
        "* Loan approval\n",
        "\n",
        "* Credit risk assessment\n",
        "\n",
        "* Fraud detection\n",
        "\n",
        "3. **Marketing & Customer Segmentation:**\n",
        "Predicting customer behavior, buying patterns, and churn.\n",
        "\n",
        "4. **E-commerce:**\n",
        "Product recommendation and predicting customer purchases.\n",
        "\n",
        "5. **HR & Recruitment:**\n",
        "Screening candidates based on job requirements and skill levels.\n",
        "\n",
        "6. **Manufacturing:**\n",
        "Identifying defective products and quality control.\n",
        "\n",
        "**Advantages of Decision Trees:**\n",
        "\n",
        "1. **Easy to understand and interpret:**\n",
        "No mathematical background needed to interpret the model.\n",
        "\n",
        "2. **Handles both numerical and categorical data**.\n",
        "\n",
        "3.  **Requires little data preprocessing:**\n",
        "No need for scaling or normalization.\n",
        "\n",
        "4. **Works well for small to medium datasets**.\n",
        "\n",
        "5. **Can capture non-linear relationships**.\n",
        "\n",
        "**Limitations of Decision Trees:**\n",
        "\n",
        "1.**Prone to overfitting:**\n",
        "Trees can become very deep and memorize the training data.\n",
        "\n",
        "2.**Unstable model:**\n",
        "Small changes in data can produce completely different trees.\n",
        "\n",
        "3. **Biased splits for features with many categories.**\n",
        "\n",
        "4. **Lower accuracy compared to ensemble methods**\n",
        "(e.g., Random Forest, Gradient Boosting)."
      ],
      "metadata": {
        "id": "vjZJeR9CBwP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "‚óè Print the model‚Äôs accuracy and feature importances\n"
      ],
      "metadata": {
        "id": "Mqs7egBtDtZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data            # Features\n",
        "y = iris.target          # Labels\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree Classifier (criterion = 'gini')\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 5. Print accuracy and feature importances\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2a7NUbLEJqq",
        "outputId": "ac85c12c-01d6-443e-f47d-30920e7b4def"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "0Fgkl2p3ERK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Decision Tree with max_depth = 3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# 4. Fully-grown Decision Tree (no depth limit)\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 5. Print comparison\n",
        "print(\"Accuracy with max_depth=3:\", acc_limited)\n",
        "print(\"Accuracy with full tree:\", acc_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKdgy3jQEtyA",
        "outputId": "1d72bcbb-9b8b-4bb4-92cc-19b238b3ba2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with full tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "‚óè Load the Boston Housing Dataset\n",
        "\n",
        "‚óè Train a Decision Tree Regressor\n",
        "\n",
        "‚óè Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "fHnag2YoEyyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing Dataset\n",
        "boston = fetch_california_housing()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions and MSE\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# 5. Print results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(boston.feature_names, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5jkv3idE83B",
        "outputId": "ced1cd24-2ac1-44a4-ae2d-89e7548d515c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "‚óè Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "yjJMaXAtFGjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the model\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. GridSearchCV for hyperparameter tuning\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                 # 5-fold cross validation\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# 6. Fit the model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 7. Best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# 8. Evaluate with best estimator\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgWCDVgCFSxE",
        "outputId": "fc6b2d73-d91b-49bc-8f0c-00879c34336e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    }
  ]
}